{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d56a726-53a8-4862-9995-e4d0695642db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training set shape: (255347, 18)\n",
      "Test set shape: (109435, 17)\n",
      "\n",
      "Data preprocessing...\n",
      "Checking missing values...\n",
      "Training set missing values:\n",
      "LoanID            0\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "Default           0\n",
      "dtype: int64\n",
      "\n",
      "Test set missing values:\n",
      "LoanID            0\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "dtype: int64\n",
      "\n",
      "Target variable distribution:\n",
      "Non-default (0): 88.39%\n",
      "Default (1): 11.61%\n",
      "Positive samples: 29653, Negative samples: 225694, Ratio: 1:7.61\n",
      "Training subset shape: (204277, 17)\n",
      "Validation subset shape: (51070, 17)\n",
      "\n",
      "Processing categorical variables...\n",
      "Column 'Education' encoded, category mapping: {\"Bachelor's\": 0, 'High School': 1, \"Master's\": 2, 'PhD': 3}\n",
      "Column 'EmploymentType' encoded, category mapping: {'Full-time': 0, 'Part-time': 1, 'Self-employed': 2, 'Unemployed': 3}\n",
      "Column 'MaritalStatus' encoded, category mapping: {'Divorced': 0, 'Married': 1, 'Single': 2}\n",
      "Column 'HasMortgage' encoded, category mapping: {'No': 0, 'Yes': 1}\n",
      "Column 'HasDependents' encoded, category mapping: {'No': 0, 'Yes': 1}\n",
      "Column 'LoanPurpose' encoded, category mapping: {'Auto': 0, 'Business': 1, 'Education': 2, 'Home': 3, 'Other': 4}\n",
      "Column 'HasCoSigner' encoded, category mapping: {'No': 0, 'Yes': 1}\n",
      "\n",
      "Handling missing values...\n",
      "\n",
      "Applying undersampling strategy...\n",
      "Training set distribution before undersampling: [180555  23722]\n",
      "Undersampling target ratio: Negative:71166, Positive:23722\n",
      "Training set shape after undersampling: (94888, 16)\n",
      "Training set distribution after undersampling: [71166 23722]\n",
      "New class ratio: 1:3.00\n",
      "\n",
      "Starting model training...\n",
      "Starting hyperparameter optimization...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "Hyperparameter optimization results:\n",
      "Best F-beta score: 0.5588\n",
      "Best parameters: {'subsample': 0.7, 'n_estimators': 500, 'min_child_weight': 2, 'max_depth': 4, 'learning_rate': 0.03, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      "Training final model with best parameters...\n",
      "\n",
      "Finding optimal classification threshold...\n",
      "Best threshold: 0.2238, F2-score: 0.4962\n",
      "At this threshold - Precision: 0.2125, Recall: 0.7447\n",
      "\n",
      "Model Evaluation:\n",
      "\n",
      "Training set evaluation (undersampled dataset):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.65      0.75     71166\n",
      "           1       0.41      0.75      0.53     23722\n",
      "\n",
      "    accuracy                           0.67     94888\n",
      "   macro avg       0.65      0.70      0.64     94888\n",
      "weighted avg       0.77      0.67      0.69     94888\n",
      "\n",
      "Training set ROC-AUC: 0.7723\n",
      "Training set F1 score: 0.5330\n",
      "Training set F2 score: 0.4962\n",
      "\n",
      "Original training set evaluation (using best threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.64      0.77    180555\n",
      "           1       0.22      0.75      0.33     23722\n",
      "\n",
      "    accuracy                           0.65    204277\n",
      "   macro avg       0.58      0.70      0.55    204277\n",
      "weighted avg       0.87      0.65      0.72    204277\n",
      "\n",
      "Original training set ROC-AUC: 0.7691\n",
      "Original training set F1 score: 0.3345\n",
      "\n",
      "Validation set evaluation (using best threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.64      0.76     45139\n",
      "           1       0.21      0.74      0.33      5931\n",
      "\n",
      "    accuracy                           0.65     51070\n",
      "   macro avg       0.58      0.69      0.55     51070\n",
      "weighted avg       0.86      0.65      0.71     51070\n",
      "\n",
      "Validation set ROC-AUC: 0.7591\n",
      "Validation set F1 score: 0.3307\n",
      "\n",
      "Prediction results exported to 'loan_default_predictions_undersampling.csv'\n",
      "\n",
      "Performing SHAP value analysis...\n",
      "\n",
      "Top 10 most important features based on SHAP values:\n",
      "1. Age: 0.5300\n",
      "2. InterestRate: 0.3781\n",
      "3. MonthsEmployed: 0.2855\n",
      "4. Income: 0.2724\n",
      "5. LoanAmount: 0.2409\n",
      "6. HasCoSigner: 0.1238\n",
      "7. HasDependents: 0.1166\n",
      "8. EmploymentType: 0.1069\n",
      "9. CreditScore: 0.1015\n",
      "10. NumCreditLines: 0.0917\n",
      "\n",
      "SHAP analysis completed and visualizations saved!\n",
      "\n",
      "Model training and evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Loan Default Prediction with SHAP Analysis\n",
    "# Using 3:1 undersampling ratio to improve detection ability for the default class\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, precision_recall_curve, f1_score, recall_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import shap  # Add SHAP library\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load Data\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "\n",
    "# 2. Data Exploration and Preprocessing\n",
    "print(\"\\nData preprocessing...\")\n",
    "\n",
    "# 2.1 Check missing values\n",
    "print(\"Checking missing values...\")\n",
    "print(\"Training set missing values:\")\n",
    "print(train_df.isnull().sum())\n",
    "print(\"\\nTest set missing values:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "# 2.2 Check data distribution\n",
    "if 'Default' in train_df.columns:\n",
    "    print(\"\\nTarget variable distribution:\")\n",
    "    default_counts = train_df['Default'].value_counts(normalize=True) * 100\n",
    "    print(f\"Non-default (0): {default_counts[0]:.2f}%\")\n",
    "    print(f\"Default (1): {default_counts[1]:.2f}%\")\n",
    "    \n",
    "    # Calculate positive-negative ratio for handling imbalance\n",
    "    neg, pos = np.bincount(train_df['Default'])\n",
    "    print(f\"Positive samples: {pos}, Negative samples: {neg}, Ratio: 1:{neg/pos:.2f}\")\n",
    "\n",
    "# 2.3 Extract features and target variable\n",
    "if 'Default' in train_df.columns:\n",
    "    X = train_df.drop('Default', axis=1)\n",
    "    y = train_df['Default']\n",
    "else:\n",
    "    X = train_df.copy()\n",
    "    y = None\n",
    "    print(\"Warning: 'Default' column not found in training set\")\n",
    "\n",
    "# Save LoanID for later use\n",
    "loan_ids = None\n",
    "if 'LoanID' in X.columns:\n",
    "    loan_ids = X['LoanID'].copy()\n",
    "\n",
    "# 2.4 Split training and validation sets - use stratified sampling to ensure consistent default ratio\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training subset shape: {X_train.shape}\")\n",
    "print(f\"Validation subset shape: {X_val.shape}\")\n",
    "\n",
    "# Handle label column in test set if it exists\n",
    "test_loan_ids = None\n",
    "if 'LoanID' in test_df.columns:\n",
    "    test_loan_ids = test_df['LoanID'].copy()\n",
    "    X_test = test_df.drop('LoanID', axis=1)\n",
    "    if 'Default' in test_df.columns:\n",
    "        y_test = test_df['Default']\n",
    "        X_test = X_test.drop('Default', axis=1)\n",
    "    else:\n",
    "        X_test = test_df.copy()\n",
    "        y_test = None\n",
    "else:\n",
    "    X_test = test_df.copy()\n",
    "    if 'Default' in X_test.columns:\n",
    "        y_test = X_test['Default']\n",
    "        X_test = X_test.drop('Default', axis=1)\n",
    "    else:\n",
    "        y_test = None\n",
    "\n",
    "# 2.5 Handle ID columns (if they exist)\n",
    "id_cols = [col for col in X_train.columns if 'ID' in col or 'Id' in col]\n",
    "for col in id_cols:\n",
    "    if col in X_train.columns:\n",
    "        X_train = X_train.drop(col, axis=1)\n",
    "    if col in X_val.columns:\n",
    "        X_val = X_val.drop(col, axis=1)\n",
    "    if col in X_test.columns:\n",
    "        X_test = X_test.drop(col, axis=1)\n",
    "\n",
    "# 2.6 Handle categorical variables\n",
    "print(\"\\nProcessing categorical variables...\")\n",
    "categorical_cols = [\n",
    "    \"Education\",\n",
    "    \"EmploymentType\",\n",
    "    \"MaritalStatus\",\n",
    "    \"HasMortgage\",\n",
    "    \"HasDependents\",\n",
    "    \"LoanPurpose\", \n",
    "    \"HasCoSigner\"\n",
    "]\n",
    "\n",
    "# Check if all categorical variables are in the dataset\n",
    "for col in categorical_cols[:]:  # Use slice to create copy to avoid modifying during iteration\n",
    "    if col not in X_train.columns:\n",
    "        categorical_cols.remove(col)\n",
    "        print(f\"Warning: Column '{col}' not in training set\")\n",
    "\n",
    "# Use LabelEncoder for categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in X_train.columns:\n",
    "        le = LabelEncoder()\n",
    "        X_train[col] = le.fit_transform(X_train[col])\n",
    "        \n",
    "        # Ensure validation and test sets use the same encoding\n",
    "        if col in X_val.columns:\n",
    "            try:\n",
    "                X_val[col] = le.transform(X_val[col])\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Validation set column '{col}' contains categories not seen in training. Will replace with most common category.\")\n",
    "                most_frequent = le.transform([X_train[col].value_counts().index[0]])[0]\n",
    "                X_val[col] = X_val[col].map(lambda x: most_frequent if x not in le.classes_ else le.transform([x])[0])\n",
    "        \n",
    "        if col in X_test.columns:\n",
    "            try:\n",
    "                X_test[col] = le.transform(X_test[col])\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Test set column '{col}' contains categories not seen in training. Will replace with most common category.\")\n",
    "                most_frequent = le.transform([X_train[col].value_counts().index[0]])[0]\n",
    "                X_test[col] = X_test[col].map(lambda x: most_frequent if x not in le.classes_ else le.transform([x])[0])\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "        print(f\"Column '{col}' encoded, category mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# 2.7 Fill missing values\n",
    "print(\"\\nHandling missing values...\")\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Use median for numeric features\n",
    "if len(numeric_cols) > 0:\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    X_train[numeric_cols] = num_imputer.fit_transform(X_train[numeric_cols])\n",
    "    X_val[numeric_cols] = num_imputer.transform(X_val[numeric_cols])\n",
    "    X_test[numeric_cols] = num_imputer.transform(X_test[numeric_cols])\n",
    "\n",
    "# Use most frequent value for categorical features\n",
    "if len(categorical_cols) > 0:\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    X_train[categorical_cols] = cat_imputer.fit_transform(X_train[categorical_cols])\n",
    "    X_val[categorical_cols] = cat_imputer.transform(X_val[categorical_cols])\n",
    "    X_test[categorical_cols] = cat_imputer.transform(X_test[categorical_cols])\n",
    "\n",
    "# 3. Apply undersampling strategy - set non-default:default = 3:1\n",
    "print(\"\\nApplying undersampling strategy...\")\n",
    "print(f\"Training set distribution before undersampling: {np.bincount(y_train)}\")\n",
    "\n",
    "# Calculate target sampling ratio - majority:minority = 3:1\n",
    "sampling_strategy = {0: int(np.sum(y_train == 1) * 3), 1: np.sum(y_train == 1)}\n",
    "print(f\"Undersampling target ratio: Negative:{sampling_strategy[0]}, Positive:{sampling_strategy[1]}\")\n",
    "\n",
    "# Apply random undersampling\n",
    "rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Training set shape after undersampling: {X_train_resampled.shape}\")\n",
    "print(f\"Training set distribution after undersampling: {np.bincount(y_train_resampled)}\")\n",
    "print(f\"New class ratio: 1:{np.sum(y_train_resampled == 0)/np.sum(y_train_resampled == 1):.2f}\")\n",
    "\n",
    "# 4. Model Training - Using undersampled data\n",
    "print(\"\\nStarting model training...\")\n",
    "\n",
    "# 4.1 Base XGBoost model initialization\n",
    "base_model = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    # Use smaller scale_pos_weight since we've already balanced data via undersampling\n",
    "    scale_pos_weight=1.0,  # Can try values between 1.0-1.5\n",
    "    random_state=42,\n",
    "    # The following parameters help with imbalanced datasets\n",
    "    max_delta_step=1,  # Helps with imbalanced data\n",
    "    min_child_weight=1  # Lower values allow algorithm to learn more specific patterns\n",
    ")\n",
    "\n",
    "# 4.2 Hyperparameter search space - Focus on improving prediction for default class\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_child_weight': [1, 2, 3],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "# 4.3 Set cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 4.4 Custom scorer - Emphasize recall\n",
    "# beta=2 makes recall weight twice as much as precision\n",
    "def f_beta_scorer(y_true, y_pred_proba, beta=2.0):\n",
    "    \"\"\"Custom F-beta score, emphasizing recall more\"\"\"\n",
    "    y_pred = (y_pred_proba >= 0.3).astype(int)  # Lower threshold favors finding default customers\n",
    "    \n",
    "    # Calculate recall and precision\n",
    "    true_pos = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    pred_pos = np.sum(y_pred == 1)\n",
    "    actual_pos = np.sum(y_true == 1)\n",
    "    \n",
    "    if pred_pos == 0 or actual_pos == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = true_pos / pred_pos\n",
    "    recall = true_pos / actual_pos\n",
    "    \n",
    "    # Calculate F-beta score, beta>1 emphasizes recall more\n",
    "    f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall + 1e-10)\n",
    "    return f_beta\n",
    "\n",
    "# Make scorer compatible with sklearn\n",
    "from sklearn.metrics import make_scorer\n",
    "f_beta_scorer_func = make_scorer(f_beta_scorer, greater_is_better=True, needs_proba=True)\n",
    "\n",
    "# 4.5 Hyperparameter search\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Increase number of iterations for better results\n",
    "    scoring=f_beta_scorer_func,  # Use custom F-beta score\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train using undersampled data\n",
    "random_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 4.6 Output best parameters and score\n",
    "print(\"\\nHyperparameter optimization results:\")\n",
    "print(f\"Best F-beta score: {random_search.best_score_:.4f}\")\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "\n",
    "# 4.7 Train final model with best parameters\n",
    "print(\"\\nTraining final model with best parameters...\")\n",
    "best_model = XGBClassifier(**random_search.best_params_, random_state=42)\n",
    "# Simplify training call to avoid version compatibility issues\n",
    "best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 5. Find optimal classification threshold - Focus on improving recall\n",
    "print(\"\\nFinding optimal classification threshold...\")\n",
    "# Generate prediction probabilities on validation set\n",
    "y_val_pred_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate precision and recall for different thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_val_pred_proba)\n",
    "\n",
    "# Calculate F-beta scores for different thresholds (beta=2, emphasize recall)\n",
    "beta = 2\n",
    "f_beta_scores = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall + 1e-10)\n",
    "\n",
    "# Find threshold with best F-beta score\n",
    "best_idx = np.argmax(f_beta_scores)\n",
    "best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "best_f_beta = f_beta_scores[best_idx]\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.4f}, F{beta}-score: {best_f_beta:.4f}\")\n",
    "print(f\"At this threshold - Precision: {precision[best_idx]:.4f}, Recall: {recall[best_idx]:.4f}\")\n",
    "\n",
    "# Visualize precision-recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, marker='.', label='PR curve')\n",
    "plt.scatter(recall[best_idx], precision[best_idx], c='red', label=f'Best threshold: {best_threshold:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve with Best Threshold (F2-score optimized)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('precision_recall_curve_undersampling.png')  # Save the chart\n",
    "plt.close()\n",
    "\n",
    "# 6. Model Evaluation\n",
    "print(\"\\nModel Evaluation:\")\n",
    "# Function to predict using custom threshold\n",
    "def predict_with_threshold(model, X, threshold=0.5):\n",
    "    \"\"\"Make predictions using custom threshold\"\"\"\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    return (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "# 6.1 Training set evaluation\n",
    "y_train_pred = predict_with_threshold(best_model, X_train_resampled, best_threshold)\n",
    "y_train_pred_proba = best_model.predict_proba(X_train_resampled)[:, 1]\n",
    "\n",
    "print(\"\\nTraining set evaluation (undersampled dataset):\")\n",
    "print(classification_report(y_train_resampled, y_train_pred))\n",
    "print(f\"Training set ROC-AUC: {roc_auc_score(y_train_resampled, y_train_pred_proba):.4f}\")\n",
    "print(f\"Training set F1 score: {f1_score(y_train_resampled, y_train_pred):.4f}\")\n",
    "print(f\"Training set F{beta} score: {f_beta_scores[best_idx]:.4f}\")\n",
    "\n",
    "# 6.2 Evaluate on original training set\n",
    "y_orig_train_pred = predict_with_threshold(best_model, X_train, best_threshold)\n",
    "y_orig_train_pred_proba = best_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "print(\"\\nOriginal training set evaluation (using best threshold):\")\n",
    "print(classification_report(y_train, y_orig_train_pred))\n",
    "print(f\"Original training set ROC-AUC: {roc_auc_score(y_train, y_orig_train_pred_proba):.4f}\")\n",
    "print(f\"Original training set F1 score: {f1_score(y_train, y_orig_train_pred):.4f}\")\n",
    "\n",
    "# 6.3 Validation set evaluation\n",
    "y_val_pred = predict_with_threshold(best_model, X_val, best_threshold)\n",
    "print(\"\\nValidation set evaluation (using best threshold):\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(f\"Validation set ROC-AUC: {roc_auc_score(y_val, y_val_pred_proba):.4f}\")\n",
    "print(f\"Validation set F1 score: {f1_score(y_val, y_val_pred):.4f}\")\n",
    "\n",
    "# 6.4 Test set evaluation (if labels are available)\n",
    "if y_test is not None:\n",
    "    y_test_pred = predict_with_threshold(best_model, X_test, best_threshold)\n",
    "    y_test_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(\"\\nTest set evaluation (using best threshold):\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(f\"Test set ROC-AUC: {roc_auc_score(y_test, y_test_pred_proba):.4f}\")\n",
    "    print(f\"Test set F1 score: {f1_score(y_test, y_test_pred):.4f}\")\n",
    "\n",
    "# 6.5 Confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Validation Set Confusion Matrix (using best threshold)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('confusion_matrix_undersampling.png')  # Save the chart\n",
    "plt.close()\n",
    "\n",
    "# 6.6 Feature importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "feature_importance = best_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), np.array(X_train.columns)[sorted_idx])\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_undersampling.png')  # Save the chart\n",
    "plt.close()\n",
    "\n",
    "# 7. Export prediction results\n",
    "if y_test is None and test_loan_ids is not None:\n",
    "    # If test set has no labels, create predictions\n",
    "    test_pred = predict_with_threshold(best_model, X_test, best_threshold)\n",
    "    test_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Create submission file\n",
    "    submission = pd.DataFrame({\n",
    "        'LoanID': test_loan_ids,\n",
    "        'Default': test_pred,\n",
    "        'DefaultProbability': test_pred_proba\n",
    "    })\n",
    "    \n",
    "    submission.to_csv('loan_default_predictions_undersampling.csv', index=False)\n",
    "    print(\"\\nPrediction results exported to 'loan_default_predictions_undersampling.csv'\")\n",
    "\n",
    "# 8. SHAP Value Analysis\n",
    "print(\"\\nPerforming SHAP value analysis...\")\n",
    "\n",
    "# Create explainer object - TreeExplainer is optimized for tree-based models like XGBoost\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "# Calculate SHAP values for validation set\n",
    "# Limit to a subset if the dataset is very large\n",
    "sample_size = min(500, X_val.shape[0])  # Use at most 500 samples to avoid memory issues\n",
    "X_val_sample = X_val.sample(sample_size, random_state=42) if X_val.shape[0] > sample_size else X_val\n",
    "shap_values = explainer.shap_values(X_val_sample)\n",
    "\n",
    "# 8.1 Summary plot - This creates the visualization like in your first image\n",
    "plt.figure(figsize=(12, 16))\n",
    "shap.summary_plot(\n",
    "    shap_values, \n",
    "    X_val_sample,\n",
    "    plot_type=\"dot\", \n",
    "    show=False,  # Don't display immediately \n",
    "    max_display=20  # Show top 20 features\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_summary_plot.png', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 8.2 Create bar plot of mean absolute SHAP values for top features\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(\n",
    "    shap_values, \n",
    "    X_val_sample,\n",
    "    plot_type=\"bar\", \n",
    "    show=False,\n",
    "    max_display=10  # Top 10 features\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_importance_bar_plot.png', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 8.3 Create detailed dependency plots for top 5 features\n",
    "# Get mean absolute SHAP values for each feature\n",
    "mean_abs_shap = np.abs(shap_values).mean(0)\n",
    "feature_importance_order = np.argsort(mean_abs_shap)[::-1]  # Sort in descending order\n",
    "top_features = feature_importance_order[:5]  # Get indices of top 5 features\n",
    "\n",
    "# Create separate dependency plots for each top feature\n",
    "for i, feature_idx in enumerate(top_features):\n",
    "    feature_name = X_val.columns[feature_idx]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.dependence_plot(\n",
    "        feature_idx, \n",
    "        shap_values, \n",
    "        X_val_sample,\n",
    "        show=False,\n",
    "        interaction_index=None  # Set to None to turn off interaction coloring\n",
    "    )\n",
    "    plt.title(f\"SHAP Dependence Plot for {feature_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'shap_dependence_{i+1}_{feature_name}.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# 8.4 Create a partial dependence plot for the most important feature (similar to your second image)\n",
    "top_feature_idx = feature_importance_order[0]\n",
    "top_feature_name = X_val.columns[top_feature_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap_values_for_top_feature = shap_values[:, top_feature_idx]\n",
    "feature_values = X_val_sample.iloc[:, top_feature_idx]\n",
    "\n",
    "# Sort points by feature value\n",
    "sorted_indices = np.argsort(feature_values)\n",
    "sorted_values = feature_values.iloc[sorted_indices]\n",
    "sorted_shap = shap_values_for_top_feature[sorted_indices]\n",
    "\n",
    "plt.scatter(sorted_values, sorted_shap, alpha=0.6)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.7)\n",
    "plt.xlabel(top_feature_name)\n",
    "plt.ylabel(f'SHAP value (impact on model output)')\n",
    "plt.title(f'Impact of {top_feature_name} on Prediction')\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_feature_dependence_plot.png', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 8.5 Print top 10 most important features based on SHAP\n",
    "print(\"\\nTop 10 most important features based on SHAP values:\")\n",
    "for i in range(10):\n",
    "    if i < len(feature_importance_order):\n",
    "        feature_idx = feature_importance_order[i]\n",
    "        feature_name = X_val.columns[feature_idx]\n",
    "        feature_importance = mean_abs_shap[feature_idx]\n",
    "        print(f\"{i+1}. {feature_name}: {feature_importance:.4f}\")\n",
    "\n",
    "print(\"\\nSHAP analysis completed and visualizations saved!\")\n",
    "print(\"\\nModel training and evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
